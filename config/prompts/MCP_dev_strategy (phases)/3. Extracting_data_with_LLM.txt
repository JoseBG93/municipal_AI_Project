# Fase 3: Extracci칩n de datos con LLM

Dise침ar un pipeline que procese documentos en distintos formatos (Word, PDF, OCR si es necesario), asegurando compatibilidad con las diversas estructuras y calidades documentales que puedan encontrarse en el entorno municipal. Este pipeline debe estar preparado para:

- Detectar autom치ticamente el tipo de archivo y su codificaci칩n.
- Aplicar tecnolog칤as de OCR (como Tesseract u otras basadas en ML) en documentos escaneados o im치genes.
- Convertir documentos en formatos uniformes (por ejemplo, pasar PDFs a texto plano estructurado o HTML intermedio si es necesario).
- Aplicar preprocesamiento riguroso: limpieza de texto (eliminaci칩n de saltos de l칤nea err칩neos, caracteres no reconocibles), identificaci칩n de secciones clave mediante patrones (regex o LLMs), supresi칩n de ruido (sellos, encabezados no relevantes, m치rgenes, pies de p치gina).
- Indexar los contenidos procesados para que sean accesibles desde otros m칩dulos del sistema, permitiendo trazabilidad y depuraci칩n de errores. Este pipeline ser치 el primer eslab칩n t칠cnico en la cadena de extracci칩n de datos, y deber치 validarse con documentos de prueba variados para garantizar su solidez.

## Usar LLMs para:

- **Detectar entidades:** extraer autom치ticamente campos relevantes como nombres, apellidos, NIF/CIF, fechas de otorgamiento, importes de las operaciones, direcciones postales, y referencias catastrales. Esto implica no solo identificar los datos, sino tambi칠n contextualizarlos dentro de su funci칩n legal en el documento.

- **Clasificar documentos y determinar su tipo:** aplicar modelos de clasificaci칩n sem치ntica que reconozcan si el documento corresponde a una compraventa, herencia, donaci칩n, o escritura p칰blica, bas치ndose en el lenguaje jur칤dico, estructura del texto y expresiones comunes.

- **Relacionar datos entre s칤 (por ejemplo, inmueble con contribuyente):** establecer v칤nculos l칩gicos y legales entre las entidades reconocidas, como asociar un inmueble con su titular, conectar los valores declarados con los sujetos fiscales, o vincular fechas relevantes a etapas del procedimiento. Esto permitir치 construir una representaci칩n estructurada y validable conforme al MCP.

- **Representar los datos extra칤dos conforme a la estructura del MCP:** garantizando que cada entidad y atributo detectado por el sistema se traduzca a una instancia bien formada del modelo previamente definido. Esta representaci칩n deber치 realizarse en formato t칠cnico compatible (como JSON o JavaScript estructurado) e incluir las relaciones entre entidades, referencias cruzadas, y validaciones previas que aseguren coherencia sem치ntica y fiscal. El GPT deber치 generar estructuras completas, con campos normalizados, etiquetas homog칠neas y jerarqu칤as claras que faciliten su posterior procesamiento o integraci칩n. Adem치s, deber치 incluir metadatos como la fuente del dato, nivel de confianza, y cualquier transformaci칩n aplicada, con el fin de mantener trazabilidad y control de calidad del sistema en su conjunto.

## Herramientas clave:

- 游냀 **Python + LangChain:** para el dise침o, implementaci칩n y orquestaci칩n de agentes inteligentes y flujos conversacionales con capacidad de razonamiento estructurado y control de contexto.
- 游 **spaCy o NLTK:** librer칤as de procesamiento de lenguaje natural (NLP) utilizadas para tokenizaci칩n, an치lisis gramatical, detecci칩n de entidades nombradas (NER), lematizaci칩n y otras tareas ling칲칤sticas previas a la extracci칩n estructurada.
- 游빔 **Pydantic o dataclasses:** para definir modelos de datos con validaci칩n estricta y tipado est치tico, facilitando la estructuraci칩n segura y coherente de los datos extra칤dos antes de exportarlos o integrarlos.
- 游댌 **OpenAI LLM (con funciones de extracci칩n estructurada):** herramienta principal de comprensi칩n sem치ntica, capaz de identificar, contextualizar y convertir informaci칩n libre en datos estructurados directamente mapeables al modelo MCP. Puede integrarse con LangChain para definir prompts espec칤ficos por tipo de documento o fase de an치lisis.

## Validaci칩n

Validar los resultados mediante sets de prueba predefinidos, dise침ados espec칤ficamente para representar distintas clases documentales, variantes estructurales y niveles de calidad del texto. Cada set deber치 incluir documentos con errores intencionados, ambig칲edades y casos l칤mite para probar la robustez del sistema. Adem치s del control de precisi칩n (exactitud en la extracci칩n de cada entidad) y cobertura (capacidad para extraer todos los campos requeridos), tambi칠n se evaluar치 la capacidad del sistema para:

- Detectar omisiones y errores sutiles.
- Informar sobre el nivel de confianza de cada extracci칩n.
- Generar logs explicativos y trazables por documento.
- Ejecutar m칠tricas cuantitativas (precisi칩n, exhaustividad, F1-score) y cualitativas (revisi칩n manual guiada por el GPT).

El GPT deber치 guiar en la creaci칩n de estos sets, proponer criterios de evaluaci칩n y ayudar en la interpretaci칩n de los resultados para iterar en la mejora del sistema.
